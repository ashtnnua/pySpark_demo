# -*- coding: utf-8 -*-
"""PySpark Tutorial

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19n-fxw15QO2amuaseLCu5i6xRP6AxMQa

## Imports and setup
"""

pip install pyspark

import pyspark
from pyspark.sql import SparkSession
# Read further to understand why these have been imported
from pyspark.sql import functions as f
from pyspark.sql.functions import lit
from pyspark.sql.functions import when

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

spark = SparkSession.builder.appName('test_pt1').getOrCreate()

# What does the spark object look like?
spark
# Well that was not helpful at all.
# Unable to click on the spark GUI. Maybe need to retry on local.

## Read the dataset
df = spark.read.csv('sample_data/test.csv', header = True)
df.show()

"""## Test out different functions"""

## Check the schema
df.printSchema()
# I do not like this schema since the age and exp is being read as string.
# Even thought he default is int in the csv.
# This is because we have not passed a proper schema. We need to ask spark to infer_schema

## Read again with inferSchema this time
df = spark.read.csv('sample_data/test.csv',header = True, inferSchema= True)
df.printSchema()
# Yay!!

## Trying to check methods and functions similar to pandas df

print(df.columns)
# print(df.shape) <- Shape attr does not exist
# print(df.describe()) # <- Describe works but gives a basic summary ## Describe has a show as well.
# print(df.head()) # <- Head gives only 1 row by default in list format.
print(df.orderBy('Age').show())
# Show has an innate print. Don't need to add that in print statement to see the ouput. The None at the end is because of print(...show()) I think...

df.select(['Age', 'nAme', 'exp']).show()

## What!!?!?!?!?!?!
## spark dataframe is not case sensitive to column names

## WHAT!!!?!?!?!?!?!!?!?

# B) Krish doesnt know this. Mwuaahaha.
# Indexing can be done using where condition like in sql
# Havent checked all functions like rank/rownum
# TODO
df.where('age>25').show()

# Using where and select together. Just trying things out
df.select(['Age','Name','exp']).where('exp >0 and age <30').show()

"""## Adding/Updating a column"""

## Adding a new column

# ***********
## withColumn

# defined as
# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.

## So, updating the column value seems easy

# To add a new column with a constant value, we need to import lit (literal constant)
# from pyspark.sql.functions import lit

df = df.withColumn('Gender',lit('Male'))
# This does not actually save it back to df
# Need to check for inplace operation or assign back to df manually

df.show()

# Add a new column based on an existing column

# To use when, you need to import it from spark sql functions
from pyspark.sql.functions import when

df = df.withColumn('has_Experience',when((df.Exp > 0), lit('Yes')) .otherwise(lit('No')))
df.show()

# What is the scene with .otherwise and not ,otherwise.
# The syntax is:
# when(cond1, val1).when(cond2, val2).otherwise(val3)

# Funny enough, the column names when used as df. Exp are case sensitive?
# Atleast in the when condition
# Yep, test to be case sensitive even outside when.

# df.exp <- Gives an error
df.Exp

# Update existing column value
# Update age for all and exp for all except "Chaitanya"

# Obviously I can do it using 2 statements. But this cannot be efficient if I am adding multiple columns.
df.withColumn('Age',df.Age+1).show()
df.withColumn('Exp',when(df.Name != 'Chaitanya',df.Exp+1).otherwise(df.Exp)).show()

# Is there a way to do it in 1 go?
# I will come back to this later.

# Coming back to this. Can be done in a single statement. Just keep calling withColumn on the withColumn output.
# Eg:
df.withColumn('Age',df.Age+1).withColumn('Exp',when(df.Name != 'Chaitanya',df.Exp+1).otherwise(df.Exp)).show()
# Bam!

"""## Drop columns"""

## Adding a dummy column so I cna drop that
df = df.withColumn('dummyCol',lit('randomTextHere'))
df.show()

df = df.drop('dummyCol')
df.show()
# or df.drop(['dummyCol'])
# Seems straightforward
# drop is also not an inplace operation. I think none of the operations in pyspark dataframe are inplace operations

"""## Renaming columns"""

df = df.withColumnRenamed('has_Experience','Experienced')
df.show()
# Tried doing it with list of columns, doesnt work
# A dict doesnt work either
# Has to be a single column name

"""## Random check"""

# What does collect() do
# From the user guide:
# Returns all the records as a list of Row.

df.withColumnRenamed('Age','Age2').collect()

df.select("Name","Age", lit(0.3).alias("num")).show()

df.select('*').show()
# Just like sql

"""## Load data again

test2.csv
"""

# Load new csv
df = spark.read.csv('sample_data/test2.csv', header = True, inferSchema= True)
df.show()

"""## Dealing with Nans

Importing test2 csv which is essentially the same csv but more obs with Nans.
Restart notebook, run imports and come back here
"""

# na.drop drops all rows with NULLs. Same as pandas I guess
df.na.drop().show()
df.na.drop(how = 'any',thresh=2).show()
# Thresh is threshhold. This implies, there should be atleast 2 non-null values, otherwise drop the row.
df.na.drop(how = 'any',thresh=4).show()
df.na.drop(how = 'any',thresh=5).show()

# Subset works the same way as pandas
# Will consider NULLs only in the columns specified under subset
df.na.drop(subset = ['Age']).show()
df.na.drop(subset = ['Age','Name']).show()

# Interesting that only numeric columns got NULLs filles with 0 and string columns got filled with '0'
df.na.fill(0).show()
df.na.fill('0').show()
# Fill also works with subset

# Offtopic, How to directly change the datatype of the column
# TODO

# Need to import functions from pyspark.sql to use aggregation functions like min/max/mean
from pyspark.sql import functions as f
df.show()
df.agg(f.sum(df.Salary)/df.count(),f.mean(df.Exp), f.mean(df.Salary)).show()
# Mean automatically ignores the NULLS

# Check imputer function
# from pyspark.sql.functions import imputer
# Skipping this for now

"""## Filter Operations"""

# df.show()

# Filter is the same as where
# Is there any difference at the backend?
df.filter('Salary>1000').show()
df.where('Salary>1000').show()

# The conditions can also be written like in pandas
df.filter((df['Salary']<1000) & (df['Age']>25)).show()

"""## Aggregation/Groupby"""

## Loading new csv

df = spark.read.csv('sample_data/test3.csv', header=True, inferSchema=True)
df.show()

# groupBy and groupby both work?
df.groupby('Name').mean().show()
df.groupby('Name').count().show()

df.agg({'Name':'count','Salary':'sum'}).show()

"""## ML library"""

from pyspark.ml.feature import VectorAssembler

df.show()

df = df.withColumn('Bonus',(df.salary*0.3 + df.salary/4))
df = df.drop('indeps')

feats = VectorAssembler(inputCols=['Bonus','salary'], outputCol='indeps')
df = feats.transform(df)
df.show()

dfp = df.toPandas()
dfp.head()

df2 = spark.createDataFrame(dfp)

df2.show()

